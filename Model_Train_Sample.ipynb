{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOGTDGqa/GA/IgY4HuoZshH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"VsK5hO6L9Jhz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Github/Music_char-rnn/"],"metadata":{"id":"rLzljHAh9ek0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQvZigVOOhhL"},"outputs":[],"source":["import os\n","import json\n","import argparse\n","import numpy as np\n","from model import build_model, load_weights\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n"]},{"cell_type":"code","source":["MODEL_DIR = './model'\n","DATA_DIR = './data'\n","LOG_DIR = './logs'\n","\n","BATCH_SIZE = 16\n","SEQ_LENGTH = 64\n"],"metadata":{"id":"EPmTlsiiOmSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_weights(epoch, model):\n","    if not os.path.exists(MODEL_DIR):\n","        os.makedirs(MODEL_DIR)\n","    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n","\n","def load_weights(epoch, model):\n","    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n","\n","def build_model(batch_size, seq_len, vocab_size):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size, 512, batch_input_shape=(batch_size, seq_len)))\n","    for i in range(3):\n","        model.add(LSTM(256, return_sequences=True, stateful=True))\n","        model.add(Dropout(0.2))\n","\n","    model.add(TimeDistributed(Dense(vocab_size))) \n","    model.add(Activation('softmax'))\n","    return model\n"],"metadata":{"id":"8-WjDRXfOohX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainLogger(object):\n","    def __init__(self, file):\n","        self.file = os.path.join(LOG_DIR, file)\n","        self.epochs = 0\n","        with open(self.file, 'w') as f:\n","            f.write('epoch,loss,acc\\n')\n","\n","    def add_entry(self, loss, acc):\n","        self.epochs += 1\n","        s = '{},{},{}\\n'.format(self.epochs, loss, acc)\n","        with open(self.file, 'a') as f:\n","            f.write(s)\n","\n","def read_batches(T, vocab_size):\n","    length = T.shape[0]; #129,665\n","    batch_chars = int(length / BATCH_SIZE); # 8,104\n","\n","    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH): # (0, 8040, 64)\n","        X = np.zeros((BATCH_SIZE, SEQ_LENGTH)) # 16X64\n","        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, vocab_size)) # 16X64X86\n","        for batch_idx in range(0, BATCH_SIZE): # (0,16)\n","            for i in range(0, SEQ_LENGTH): #(0,64)\n","                X[batch_idx, i] = T[batch_chars * batch_idx + start + i] # \n","                Y[batch_idx, i, T[batch_chars * batch_idx + start + i + 1]] = 1\n","        yield X, Y\n","\n","def train(text, epochs=100, save_freq=10):\n","\n","    # character to index and vice-versa mappings\n","    char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n","    print(\"Number of unique characters: \" + str(len(char_to_idx))) #86\n","\n","    with open(os.path.join(DATA_DIR, 'char_to_idx.json'), 'w') as f:\n","        json.dump(char_to_idx, f)\n","\n","    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n","    vocab_size = len(char_to_idx)\n","\n","    #model_architecture\n","    model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n","    model.summary()\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","    #Train data generation\n","    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int32) #convert complete text into numerical indices\n","\n","    print(\"Length of text:\" + str(T.size)) #129,665\n","\n","    steps_per_epoch = (len(text) / BATCH_SIZE - 1) / SEQ_LENGTH  \n","\n","    log = TrainLogger('training_log.csv')\n","\n","    for epoch in range(epochs):\n","        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n","        \n","        losses, accs = [], []\n","\n","        for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n","            \n","            print(X);\n","\n","            loss, acc = model.train_on_batch(X, Y)\n","            print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n","            losses.append(loss)\n","            accs.append(acc)\n","\n","        log.add_entry(np.average(losses), np.average(accs))\n","\n","        if (epoch + 1) % save_freq == 0:\n","            save_weights(epoch + 1, model)\n","            print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))\n"],"metadata":{"id":"-NsRmh-ePSWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_model(16, 64, 50)\n","model.summary()\n"],"metadata":{"id":"7HJNmN1NOtGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(open(os.path.join(DATA_DIR, 'input.txt')).read())\n"],"metadata":{"id":"dqkIX0_hPpw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_sample_model(vocab_size):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size, 512, batch_input_shape=(1, 1)))\n","    for i in range(3):\n","        model.add(LSTM(256, return_sequences=(i != 2), stateful=True))\n","        model.add(Dropout(0.2))\n","\n","    model.add(Dense(vocab_size))\n","    model.add(Activation('softmax'))\n","    return model\n","\n","def sample(epoch, header, num_chars):\n","    with open(os.path.join(DATA_DIR, 'char_to_idx.json')) as f:\n","        char_to_idx = json.load(f)\n","    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n","    vocab_size = len(char_to_idx)\n","\n","    model = build_sample_model(vocab_size)\n","    load_weights(epoch, model)\n","    model.save(os.path.join(MODEL_DIR, 'model.{}.h5'.format(epoch)))\n","\n","    sampled = [char_to_idx[c] for c in header]\n","    print(sampled)\n","    \n","\n","    for i in range(num_chars):\n","        batch = np.zeros((1, 1))\n","        if sampled:\n","            batch[0, 0] = sampled[-1]\n","        else:\n","            batch[0, 0] = np.random.randint(vocab_size)\n","        result = model.predict_on_batch(batch).ravel()\n","        sample = np.random.choice(range(vocab_size), p=result)\n","        sampled.append(sample)\n","\n","    return ''.join(idx_to_char[c] for c in sampled)\n"],"metadata":{"id":"uI_AyOvF9IUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sample(10, '', 512))"],"metadata":{"id":"AZAlc9ZVAlGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sample(100, '', 512))"],"metadata":{"id":"OCgTw_JOCuR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YS45oV0PDu91"},"execution_count":null,"outputs":[]}]}